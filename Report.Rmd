---
title: "EC349 Individual Project"
output: html_document
date: "`r Sys.Date()`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Data Science Methodology

For this project I have selected John Rollins' General DS Methodology. 

The reasons for that are as follows: 
* Separate emphasis on analytic approach. Deep exploration of analytics and examining of data before preparing it is valuable when we have multiple datasets where different variables could have an impact on user reviews. It is crucial to examine any preliminary trends and correlations to see which variables are more relevant, and which ones may create unneccessary noise that will limit predictability.
* Explicit separation of data requirements and collection, unlike in CRISP-DM which has a broad Data understanding stage. Useful for analysis of types of data needed, which is relevant for handling Yelp's diverse and complex datasets.
* Tailored data preparation. CRISP-DM may not delve as deeply into specific difficulties in multifaceted datasets. This is important as Yelp's datasets are complex and have unstructured text and different business and user attributes.
* CRISP-DM and others may be closer to industry approach, whereas this project is for academic purposes.

I applied this method in the following ways:
* 

## Business Understanding

The goal is to predict user reviews (the amount of stars given by a user to a business) based on data from Yelp. 

## Analytic Approach

Firstly, it is important to see how the data can be used to make that prediction. This can be done through examining trends and correlations between stars and other variables. 

# Descriptive analytics

I lay the foundation for predictive modelling by examining relevant data. For example, I examine the distributions and whether data is skewed.

Firstly, examine the variable of interest: 'stars' in review_data_small.
The mean is 3.748 and median is 4, suggesting that ratings overall are skewed towards the higher end of the 1 to 5 scale.
```{r}
summary(review_data_small$stars)
```
Additionally, in this dataset, 'useful', 'funny', and 'cool' indicate the number of times other users found the review useful, funny, or cool, which may be relevant. As these are direct reactions to the review, this may be more relevant than, for example, user-level counts of 'useful', 'funny', and 'cool' in the user_data_small dataset.
```{r}
summary(review_data_small$useful)
summary(review_data_small$funny)
summary(review_data_small$cool)
```
Given that both the 1st quartile and median for all three is 0, as well as that 3rd quartile for two of them is 0, suggests that there is a high level of sparsity in the feedback data.

Considering the large number of reviews don't receive feedback, it is useful to look other data that may predict stars in reviews, outside of the review_data_small dataset.

For example, user history may affect the stars they leave. This includes review_count and average_stars in user_data_small.

```{r}
summary(user_data_small$review_count)
summary(user_data_small$average_stars)
```
The review_count data ranges broadly, with a small number of users who are highly active. The more active a user is, the more nuanced their reviews may be and they may have consistent rating behaviours, which could affect 'stars'. 
The average_stars portrays a slight skew in distribution, where most users rate positively. This data can provide context on a user's rating behaviour, which may predict their future ratings. 

Visualising the distribution of Stars Ratings confirms the skewness. The most popular rating is 5, followed by 4 and then 1. This may be explained by the fact that people tend to leave reviews when they had a particularly positive experience, or a very negative one. There may be a bias towards positive reviews in the data. 
```{r}
# Stars histogram
ggplot(review_data_small, aes(x=stars)) +
  geom_histogram(binwidth=1, fill='blue', color='black') +
  ggtitle("Distribution of Stars Ratings")
```

```{r}
# Review count and average stars histograms
library(ggplot2)

ggplot(user_data_small, aes(x = review_count)) +
  geom_histogram(binwidth = 1, fill = "blue", color = "black") +
  ggtitle("Distribution of Review Count") +
  xlab("Review Count") +
  ylab("Frequency")

ggplot(user_data_small, aes(x = average_stars)) +
  geom_histogram(binwidth = 0.1, fill = "green", color = "black") +
  ggtitle("Distribution of Average Stars") +
  xlab("Average Stars") +
  ylab("Frequency")
```
The histograms for review count and average stars confirm previous observations about skewness. 
For average stars, the distribution indicates that users are more likely to have extreme ratings (either very high or very low). 
The review count has a long-tail distribution with a small number of users having a high number of reviews and a vast majority having small number of reviews. For predicting stars, the number of reviews may be less predictive for users with fewer reviews since the average rating could be significantly influenced by a small number of extreme reviews.

# Diagnostic Analytics

It is important to consider **why** the star ratings are a certain way, which we can do by also including business_data into our analysis and seeing correlations between different variables. 

Correlations between stars and other variables in review_data_small are quite weak, suggesting these may be poor predictors of 'stars'. However, this does not mean they have no predictive power at all and they may still contribute to the model.
```{r}
correlation_useful <- cor(review_data_small$stars, review_data_small$useful, use = "complete.obs")
correlation_funny <- cor(review_data_small$stars, review_data_small$funny, use = "complete.obs")
correlation_cool <- cor(review_data_small$stars, review_data_small$cool, use = "complete.obs")
print(correlation_useful)
print(correlation_funny)
print(correlation_cool)
```

To calculate correlation vectors between 'stars' and other datasets, I rename any conflicting columns and merge review_data_small with each other dataset that contains int data based on user_id.

When merging review and user data, the following correlations between 'stars' and variables in user data can be observed: 
```{r}
print(corr_user)
```
Here most variables also have a relatively weak correlation, except for average_stars, which have a correlation of 0.58. 

Correlation coefficient between 'stars' and variables in 'compliment_count' in tip_data:
```{r}
print(corr_tip)
```

In business data, there are a lot of variables and while all of these could improve prediction, more flexible models reduce bias but increase variance and MSE. Thus, it may be important to focus more on variables like attributes within business, as opposed to latitude or longitude. 

For numerical variables in business_data, business_stars seems to have a strong correlation with 'stars'. 

```{r}
print(corr_numerical_business)
```

I also look at categorical variables that include 'True' and 'False' and some other simple categories. Upon examining the dataset, some attributes (such as HairSpecializesIn) have more than a 100 unique categorisations, so I focus only on those that have a few, such as 'True' and 'False', such as Open24Hours. Many of them also have 'None' which I convert to NA.
```{r}
print(corr_binary_business)
```
The correlations coefficients suggest an overall weak to moderate relationship between these business attributes and stars. 

Most variables across datasets have a mixed and small relationship with stars. Hence, our next step would be to take Predictive Analytics approach. 

## Data Understanding & Organisation

# Data Requirements
Since it is hard to establish relationships based on descriptive and diagnostic approaches, I use some data from more than one dataset. I definitely include data that had strong/moderate correlations. I also use other data since correlation does not imply causation and other variables may still serve predictive power. However, I focus more on data that is likely to be relevant, such as informaiton about the user, rather than check-in data, to reduce variance and manage complexity.

# Data Collection

Secondary data is collected on Yelp. Due to crashing and no progress in installation of original datasets, small datasets are used.

Some variables have incomplete or unavailable data. 

# Data Understanding 
There are 5 datasets.

# Data Preparation

## Validation & Deployment

# Modelling
In order to minimise the risk of overfitting (and thus improve predictability) and computational resources overload, I start with a model with few predictors that are hypothetically likely to impact 'stars'. This will also allow me to understand the impact of each new variable I add iteratively. 

The variables that have had strong or moderate correlation (bigger than 0.1) with 'stars' may have good predictive power. These include:
*average_stars in user_data_small (correlation coefficient of 0.58)
*business_stars (0.49), DriveThru (-0.21), and Open24Hours (-0.26) in business_data

These variables could predict 'stars' due to various explanations:
*average stars of a user may be a strong indicator of their rating behaviour as it is possible that users who generally give higher/lower ratings will continue to do so
*a user's rating may align closely with the general consensus of other users represented by business stars
*DriveThru may have negative correlation with stars potentially due to the type of business being of poorer quality, such as fast food restaurants
*Open24Hours may be negatively correlated with stars due to poorer quality of service during off-peak hours, providing insights into how operational hours may impact user perceptions




Supervised learning with shrinkage methods. OLS estimator would perform poorly in MSE due to high variance and overfitting. Since we care about predicting and not interpretability.

We have three methods to pick from: 
OLS - no bc
Ridge - yes bc
LASSO - no bc
Possible to combine ridge and LASSO but no - why not? 

To pick the penalisation parameter, I perform validation

# Evaluation 

# Deployment

# Feedback

## Data and Method

There are 5 datasets. We observe the number of starts given by a user, which means we can perform supervised learning.

While it is possible to only use the data from User Reviews and make interpretation easier, we may have an issue of underfitting as the model is too simple (e.g. linear, when dataset follows a curved distribution), which may worsen our prediction. If we use all data that is available, it may lead to a flexible model which can fit the data better. However, as the model is made more complicated, variance will increase and it may start fitting noise, increasing Mean-Squared Error (MSE). Therefore, there is another trade-off between bias and variance. 

It is important to pick data that is most relevant 

## Biggest challenge

Preparing data. Business data was especially complex, with many variables in character form when they can be in binary form, and some nested sections which required extracting to perform analysis. 